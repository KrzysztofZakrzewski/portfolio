{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Hello and welcome to my portfolio.","text":"<p>My github https://github.com/KrzysztofZakrzewski.</p>"},{"location":"#it-is-about","title":"It is About:","text":"<p>This section contains exploratory data analyses (EDA), Python projects, applications of artificial intelligence and machine learning, as well as small-scale applications.</p>"},{"location":"#about-me","title":"About me:","text":"<p>I\u2019m passionate about exploring the world of data and technology, constantly seeking new knowledge and inspiration. As an aspiring Data Scientist, I enjoy uncovering stories hidden within data through analysis and statistics.</p> <p>I\u2019m proficient in Python and key data science libraries like Pandas, Matplotlib, and Seaborn. I regularly work with tools such as Jupyter Notebook, Anaconda, and VSCode to build and test solutions efficiently. Additionally, I\u2019m taking my first steps in Linux, expanding my technical toolkit.</p> <p>Problem-solving and learning new technologies feel like solving puzzles to me \u2014 all it takes is a text editor, a jupyter notebook, and curiosity.</p>"},{"location":"Job_offer_from_JustJoinIT/","title":"Data Analysis EDA of JustJoinIt: Domain Exploration","text":"<p>This is an analysis I carried out as part of a project in the Data Scientist: From Zero to AI course. I have to admit\u2014it really challenged me. It wasn\u2019t exceptionally difficult, but it was demanding and required a lot of careful work.</p> <p>Could it have been done better?</p> <p>Yes.</p> <p>Could the data have been analyzed in even greater detail?</p> <p>Yes.</p> <p>Does it still make an impression?</p> <p>In my opinion\u2014yes.</p> <p>One of the most interesting things I learned during this project was Cram\u00e9r\u2019s V matrix. Since the Pearson correlation matrix and its variations did not provide satisfactory results, I was forced to look for an alternative solution.</p> <p>That\u2019s where Cram\u00e9r\u2019s V came in. It is used to measure the strength of association between two categorical variables. It is essentially the equivalent of Pearson\u2019s correlation, but designed for categories rather than continuous numerical values.</p> <p>Created 06.01.2025</p> <p>Download Notebook DataSet DataSet Author</p> <p></p>"},{"location":"M_Cal/","title":"M_cal. FitTrack AI \ud83c\udfcb\ufe0f\u200d\u2642\ufe0f","text":""},{"location":"M_Cal/#an-application-for-tracking-expenses-and-calories-purchased-at-mcdonalds","title":"An application for tracking expenses and calories purchased at McDonald\u2019s. \ud83c\udf54","text":""},{"location":"M_Cal/#proof-of-concept","title":"( Proof of concept )","text":""},{"location":"M_Cal/#the-application-was-created-for-educational-purposes-only","title":"The application was created for educational purposes only.","text":"<p>Added 09.11.2025</p>"},{"location":"M_Cal/#simple-description","title":"Simple Description","text":"<p>An application that, based on photos of receipts and using the AI model \"GTP-4o-mini\", provides information about the user's expenses and calorie intake. It allows filtering of the data from various perspectives:</p> <ul> <li> <p>individual products,</p> </li> <li> <p>quantities of purchased products,</p> </li> <li> <p>prices of individual products,</p> </li> <li> <p>total receipt amount,</p> </li> <li> <p>date,</p> </li> <li> <p>and place of purchase.</p> </li> </ul> <p>After filtering the data and adding information from the user, it can be sent to the AI model, which will create a simplified workout plan that can be saved as a PDF and optionally implemented.</p> <p>The application also creates a kind of database, allowing users to save and upload CSV files previously created with it and add new data, enabling the user to track their expenses and calories in individual months.</p> <p>The application also retrieves a PDF containing the nutritional values table from McDonald's Poland. It processes the PDF and creates a usable calorie table in JSON format.</p> <p>Pervormeance Video Tech Video GitHub</p>"},{"location":"M_Cal/#installation-and-setup","title":"Installation and Setup","text":"<pre><code>Create the main project folder cd m_cal\n\nClone the repository git clone https://github.com/twoj-nick/fittrack-ai.git\n\nOr using Conda: conda env create -f environment.yml conda activate m_cal\n\nUsing terminal type `streamlit run app.py`\n</code></pre> <p>Alternative dependencies pip install -r requirements.txt</p>"},{"location":"M_Cal/#features","title":"Features","text":"<p>\ud83d\udcc4 Ability to download a PDF file containing the \u201cNutritional Values Table\u201d from the McDonald\u2019s website.</p> <p>\u270d\ud83c\udffb Creation of a JSON file with calorie and nutritional values for individual products extracted from the nutritional table PDF, or adding new entries to an existing file.</p> <p>\ud83d\udcf7 Reading information from receipt images and automatically generating a DataFrame using AI.</p> <p>\ud83d\udcc4 Loading an existing user DataFrame and adding new data from scanned receipts.</p> <p>\ud83d\udcc4 Parsing receipt data and merging it with nutritional information extracted from the PDF using AI.</p> <p>\ud83d\udcca Filtering of user data based on selected parameters.</p> <p>\ud83d\udcca Visualization of data through interactive charts.</p> <p>\ud83d\udcbe Export of filtered data to CSV or Excel formats.</p> <p>\ud83d\udcbe Option to rename filtered datasets before export.</p> <p>\ud83e\udd16 Generation of a personalized AI-based training plan.</p> <p>\ud83d\udcbe Saving the generated training plan as a PDF.</p>"},{"location":"M_Cal/#project-goal","title":"Project Goal:","text":"<p>The goal of the project was to create an application that works with real data and addresses the problems arising from it.</p> <p>Did I completely solve this problem?</p> <p>No.</p> <p>I managed to realize how big this challenge is and how a large corporation can tightly protect its data, even though it is publicly available.</p> <p>The resources required to create a fully functional application are like the 'Holy Grail' \ud83c\udfc6\u2014not everyone has access to them.</p>"},{"location":"M_Cal/#challenges","title":"Challenges:","text":""},{"location":"M_Cal/#1-data-parsing","title":"1. Data Parsing","text":"<p>Extracting data from a receipt photo was not the most difficult part. Parsing the data proved to be much more complicated.</p> <p>Examples \u2013 product information on a receipt:</p> <ul> <li> <p>\"FL_Wan_Lio_Czek\" \u2013 \"McFlurry with vanilla KitKat flavor and chocolate topping\"</p> </li> <li> <p>\"WrapChrup Klas\", \"Wpar Chrup Klas\" \u2013 only 2 parses for the product \"McWrap Crispy Classic\"</p> </li> </ul> <p>AAnd there are hundreds of such parses. There are combo meals, extended combos, individual products, add-ons, etc.</p> <p>I limited myself to a few dozen. I do not have, and even if I did, I would not show access to the files that would allow full parsing!</p>"},{"location":"M_Cal/#2-extracting-data-from-the-pdf-with-the-nutritional-values-table","title":"2. Extracting Data from the PDF with the Nutritional Values Table","text":"<p>The PDF officially available on McDonald's website is not a table, it is TEXT. Additionally, it contains a lot of information. Sending all this information to an LLM would make the prompt too large\u2014it would get cut off.</p> <p>Chunking also failed\u2014I was getting jumbled responses, as the LLM received information delivered without a sensible structure and mixed up the numbers.</p> <p>ONLY the creation of the function new_caloris_table_from_pdf_json in the file src/pdf_parser/pdf_parser.py, which took the text as an argument, created markers as strings, then from the marker STRING A to the marker STRING B: listed product names, took 5 numbers, created key-value pairs, and saved them to JSON; then continued from STRING B to STRING C, etc., finishing at a specified string as the last key-value pair.</p>"},{"location":"M_Cal/#3-what-to-display-what-to-filter","title":"3. What to display, what to filter?","text":"<p>From an end-user perspective, I think there are too many filters.</p> <p>From a Data Scientist perspective, I think it\u2019s fine.</p>"},{"location":"M_Cal/#4-creating-an-application-that-upon-launch-automatically-generates-all-the-necessary-files-and-folders","title":"4. Creating an application that, upon launch, automatically generates all the necessary files and folders","text":"<p>The application creates:</p> <ul> <li> <p>a \"main_dataframe\" folder with a 'main.csv' file as a template, which we modify and save data to; its name can also be changed.</p> </li> <li> <p>a \"logs\" folder with logs.log containing logs of scraping the official PDF.</p> </li> <li> <p>a \"pdf\" folder where the official PDF is saved.</p> </li> <li> <p>a \"json_calories_table\" folder where the file \"offer_classic.json\" is created, which is used later.</p> </li> <li> <p>a \"receipt\" folder for receipt photos.</p> </li> <li> <p>a \"temporary_json_from_receipt\" folder with a temporary JSON file obtained from the receipt.</p> </li> <li> <p>a \"temporary_json_parsed\" folder with a temporary parsed JSON file, where information is saved to the dataframe.</p> </li> </ul>"},{"location":"M_Cal/#5-creating-a-modular-application","title":"5. Creating a modular application.","text":"<p>Developing an application with multiple modules that work together.</p> <p>It was successful \ud83d\ude0a</p>"},{"location":"M_Cal/#6-writing-a-letter","title":"6. Writing a letter \ud83d\udc8c","text":"<p>Creating the application in such a way that function names are clear, logical, self-explanatory, and the workflow can be easily followed.</p> <p>Did it succeed? I don\u2019t know, it\u2019s not for me to judge. \ud83e\udd14</p> <p>But it is definitely not absolute spaghetti.</p>"},{"location":"M_Cal/#weak-points","title":"Weak points","text":"<ul> <li> <p>In its current form, the application is not suitable for deployment in the cloud. I would have to give up parsing. It also dynamically creates files and folders\u2014I would need to either give that up or save and read them from buckets \ud83e\udea3.</p> </li> <li> <p>Due to the limited parsing of products adapted for operation in a large-scale system, the application may sometimes skip certain products as duplicates, even within its limited functiona</p> </li> </ul>"},{"location":"M_Cal/#project-structure","title":"Project Structure","text":"<pre>\nm_cal/\n\u251c\u2500\u2500 app.py # Main Streamlit application file\n\u251c\u2500\u2500 src/\n\u2502 \u251c\u2500\u2500 pdf_parser/ # Modules for PDF parsing\n\u2502 \u251c\u2500\u2500 data/ # Data export (CSV, Excel, PDF)\n\u2502 \u251c\u2500\u2500 ai_trainer/ # AI-powered training plan generation\n\u2502 \u251c\u2500\u2500 pltos/ # Visualizations and charts\n\u2502 \u2514\u2500\u2500 utils/ # Utility and helper functions\n\u2502\n\u251c\u2500\u2500 json_calories_table/ # Static calorie table in JSON format\n\u251c\u2500\u2500 logs/ # Application logs\n\u251c\u2500\u2500 main_dataframe/ # Template for base dataframe\n\u251c\u2500\u2500 pdf/ # Nutrition table PDFs\n\u251c\u2500\u2500 receipt/ # Dynamic receipt image storage\n\u251c\u2500\u2500 temporary_json_from_receipt/ # Temporary JSONs generated from receipts\n\u251c\u2500\u2500 parsed_json_for_user_dataframe/ # Final parsed JSONs for dataframe creation\n\u2502\n\u2514\u2500\u2500 requirements.txt # Python dependencies</pre>"},{"location":"M_Cal/#mini-spec","title":"Mini spec:","text":"<pre><code>python 3.11\npandas\npathlib\nstreamlit\nnumpy\nlogging\nurllib.parse\ndotenv\nopenai\nopenpyxl\nfpdf2\npdfplumber\nplotly.express\nrequests\n\nenvironment.yam and requirements.txt are in github\n</code></pre> <p>Linkedin</p>"},{"location":"M_Cal/#email","title":"\ud83d\udce7 email:","text":"<p>krzysztof.zakrzewski@protonmail.com</p>"},{"location":"M_Cal/#appearance-of-the-application","title":"Appearance of the application","text":""},{"location":"M_Cal/#me-python-and-data-memeified","title":"Me, Python and Data - memeified.","text":""},{"location":"W_przygotowaniau/","title":"Next projects will appear soon. \u270d\ud83c\udffb","text":""},{"location":"coffee/","title":"Data Analysis EDA of Coffee: Domain Exploration","text":"<p>I present an EDA analysis based on coffee-related data.</p> <p>My personal goal in this small project, besides performing the analysis itself, was to carry it out on a Linux system. The experience had its challenges, but it was not exceptionally difficult.</p> <p>During this analysis, I explored the differences between time in Python and time in pandas. It can really give you a headache\u2014time is not always equal to time.</p> <p>Additionally, I decided to answer the question of whether the data comes from the USA, by comparing the results with the expected sales increases or decreases.</p> <p>It turned out that the data is either synthetic or comes from another region of the world. Where exactly? I don\u2019t know\u2014maybe someday I\u2019ll find the time to find out. \ud83d\ude42</p> <p>Enjoy the \u2615!</p> <p>Created 09.09.2025</p> <p>Download Notebook Link to DataSet Link to author of Data</p> <p></p>"},{"location":"iris/","title":"Data Analysis EDA of Irises: Domain Exploration","text":"<p>Welcome to explore my project on a classic exploratory data analysis (EDA) topic \u2014 the irises. In this project, you will find plenty of insightful conclusions and interesting observations regarding the physical features of these flowers, which will help you learn how to distinguish them from each other. Get ready for a fascinating journey through the data that will surely enrich your knowledge and inspire further research.</p> <p>And maybe you\u2019ll even decide to start growing them.</p> <p>Created 30.06.2025</p> <p>Download Notebook</p> <p></p>"},{"location":"rich_500/","title":"Data Analysis EDA related to the \"The 500 richest businessmen in the world\"","text":"<p>I invite you to my analysis of the 500 richest people in the world. The data pertains to the year 2024.</p> <p>The purpose of this analysis was not just curiosity about how the richest people get wealthy. Honestly, I care little about that. The goals outlined in the analysis may seem fairly standard, but they can prompt some reflection. The sums presented in the analysis are extremely unbelievable, even absurd.</p> <p>This can provoke thought. In my opinion, money is a certain kind of driving force, and the amounts shown here are simply incredible. Let\u2019s start with an observation that not everyone might realize:</p> <p>1,000,000 seconds equals approximately 11.57 days. 1,000,000,000 seconds is roughly 31 years and 8.5 months!!!</p> <p>While I can roughly predict what I\u2019ll be doing in two weeks down to the minute, predicting what I\u2019ll be doing in 31 years\u2026 I can only hope that I\u2019ll still be healthy, young, wealthy, and living my life in a certain way\u2014e.g., training on a mook yan jong. \ud83d\ude0a</p> <p>While I can imagine the legendary \u201cfirst million,\u201d the \u201cfirst billion\u201d is quite abstract. It\u2019s not a house with a backyard and a car. It\u2019s infrastructure and ecosystem. It\u2019s on a completely different scale.</p> <p>Regarding the regions, the original dataset did not meet my expectations. Division into individual countries was too detailed, while a simple division into continents seemed incomplete. It did not give full control over the data.</p> <p>I divided the countries not only into continents but also into China and India, because, in my opinion, these countries are not only huge, but in their vastness they also represent separate cultures and ecosystems. In China alone, according to the dataset, the number of billionaires is almost equal to the rest of Asia, while in India it is about half compared to China or Asia. You could divide both of these large countries into districts, but they are still vast national agglomerations, in a sense even civilizational. Their populations can already be counted in billions.</p> <p>I am providing both the original dataset and the version modified by me for analysis. As we can observe, some of the values presented in the data are extreme.</p> <p>Created 20.08.2025 </p> <p>Download Notebook Dowland Dataset</p> <p>Author of orginal dataset:</p> <p>Author of dataset Orginal Dataset</p> <p></p>"},{"location":"rich_500_app/","title":"the_500_richest \u2013 application based on a previous analysis","text":"<p>I decided to revisit and refresh one of my older codebases. It is a simple application used to present data from a previous analysis:</p> <p>rich_500.</p>"},{"location":"rich_500_app/#the-analysis-itself-is-not-the-main-focus-of-this-presentation-the-refactoring-process-is","title":"The analysis itself is not the main focus of this presentation \u2014 the refactoring process is.","text":"<p>This is also the first application I have deployed to a production environment on my own server (Raspberry Pi 5 running Ubuntu Server 24.04 LTS), using Docker and Cloudflare.</p>"},{"location":"rich_500_app/#refactoring-assumptions","title":"Refactoring Assumptions:","text":"<ul> <li> <p>Refactored a monolithic application (1,793 lines in a single file with heavy if/else logic) into a modular structure.</p> </li> <li> <p>Reduced the main application file to ~60 lines by extracting responsibilities into separate modules.</p> </li> <li> <p>Introduced 12 new files to improve separation of concerns and maintainability.</p> </li> <li> <p>Although the total line count increased to ~1,865 lines, a significant portion consists of comments, spacing, and documentation.</p> </li> <li> <p>The effective amount of executable code was reduced to approximately 1,000 lines, with further optimization still possible.</p> </li> <li> <p>The resulting code base is much more readable, easier to maintain and extend than before. \ud83d\ude04</p> </li> </ul> <p></p>"},{"location":"rich_500_app/#architecture","title":"Architecture","text":"<pre><code>app/\n\u2502\n\u251c\u2500\u2500 app.py                  # entry point (Streamlit)\n\u2502\n\u251c\u2500\u2500 loader/\n\u2502   \u2514\u2500\u2500 csv_loader.py           # load danych\n|\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500top_rich2024_ready.csv   # csv data\n|\n\u251c\u2500\u2500 analysis/\n\u2502   \u251c\u2500\u2500 data_overwview.py       # universal compiuting for basic data overview\n\u2502   \u251c\u2500\u2500 statistics_for_countries.py # descriptive statistics for countrys analysis\n\u2502   \u2514\u2500\u2500 statistics_for_global.py    # descriptive statistics for global analysis\n\u2502\n\u2502\n\u251c\u2500\u2500 visualization/\n\u2502   \u251c\u2500\u2500 plots.py            # plots for global analysis\n\u2502   \u2514\u2500\u2500 plots_countrys.py   # plots for countrys analysis\n\u2502\n\u251c\u2500\u2500 ui/\n\u2502   \u251c\u2500\u2500 components          # sidebar global an countrys\n|   |   \u2514\u2500\u2500 data_overview.py  # render universal view for basic data overview\n|   |\n\u2502   \u251c\u2500\u2500 sidebar.py          # sidebar uniwersal for all        \n\u2502   \u251c\u2500\u2500 components.py          # sidebar uniwersal for all        \n|   |   |\n|   |   \u2514\u2500\u2500 data_overview.py   # basic overview for global and coutrys analysis\n|   |\n\u2502   \u251c\u2500\u2500 pages.py/           # page/tab logic\n\u2502   |   |\n\u2502   |   \u251c\u2500\u2500 about.py        # Descryption of app\n|   |   \u251c\u2500\u2500 global_analis.py    # global analysis\n\u2502   |   \u251c\u2500\u2500 about.py        # Descryption of app\n|   |   \u2514\u2500\u2500 countrys.py     # anasysis for countrys\n|   |\n|   \u2514\u2500\u2500styles.py            # Css style for all app\n|\n\u2502\n\u2514\u2500\u2500 requirements.txt\n</code></pre> <p>Created 20.01.2026</p> <p>App link</p> <p>All changes were implemented based on the original dataset without altering their values or substantive content.</p> <p>The changes included:</p> <p>Converting numeric values written in shorthand (e.g., \"447B\") into readable numeric formats to facilitate calculations and readability.</p> <p>Removing unnecessary spaces from column names to simplify work.</p> <p>No DataFrame altered the original values.</p>"},{"location":"rich_500_app/#author-of-orginal-dataset","title":"Author of orginal dataset:","text":"<p>Author of dataset Orginal Dataset Changed Dataset</p>"},{"location":"titanic/","title":"Data Analysis EDA related to the Titanic disaster: Domain Exploration","text":"<p>Welcome to explore my second project of exploratory data analysis (EDA) related to the Titanic disaster. In this project, besides the data analysis itself, you will also find several technical aspects.</p> <p>Enjoy!</p> <p>Created on 28.08.2025</p> <p>Download Notebook Download CSV Data</p> <p></p>"},{"location":"typemachine/","title":"Simple app for generating texts: TypeMachine \ud83d\udcc4\ud83d\udd8b\ufe0f","text":""},{"location":"typemachine/#description","title":"Description","text":"<p>Typemachine is a Streamlit-based application for generating subtitles from short video files.</p> <p>The app extracts audio from a video, transcribes speech to text using OpenAI, allows basic text editing, translates the transcript into a selected language, and exports subtitles in SRT format.</p> <p>Due to technological limitations, the size of the processed video is limited to 200MB.</p>"},{"location":"typemachine/#features","title":"\u2728 Features","text":"<ul> <li>Upload short video files</li> <li>Extract audio using ffmpeg</li> <li>Speech-to-text transcription (OpenAI)</li> <li>Editable transcript</li> <li>Translation into a target language</li> <li>Export original and translated subtitles as .srt</li> </ul>"},{"location":"typemachine/#quick-start","title":"Quick start","text":"<p>\ud83d\ude80 Installation &amp; Setup 1\ufe0f\u20e3 Clone the repository git clone https://github.com/your-username/typemachine.git cd typemachine</p> <p>Switch to branch v2-lang-modules: git checkout v2-lang-modules </p> <p>2\ufe0f\u20e3 Create Conda environment (Make sure you have Conda installed (Anaconda or Miniconda)).</p> <p>conda env create -f environment.yml</p> <pre><code>Activate the environment:\n</code></pre> <p>conda activate typemachine</p> <p>3\ufe0f\u20e3 Environment variables</p> <p>Create a .env file in the project root directory:</p> <p>OPENAI_API_KEY=your_openai_api_key_here</p> <p>If not type the openai key in start of applicatin</p> <p>4\ufe0f\u20e3 Run the application streamlit run app/main.py</p> <p>The application will be available at:</p> <p>http://localhost:8501</p>"},{"location":"typemachine/#instructions-of-use","title":"Instructions of use:","text":"<p>After entering your OpenAI key, you (the user) can:</p> <ol> <li>In the \"Upload Video\" field, upload the video from which you want to extract text.</li> <li>Then press the \"Generate Audio\" button and check if the audio was generated correctly.</li> <li>Next, press the \"Audio Transcription\" button \u2014 the text from the uploaded video will appear.</li> <li>You can modify it, but remember to press CTRL + ENTER to confirm your changes.</li> <li>Enter the language you want to translate the script into (by default, it will be Polish).</li> <li>Press the \"Translate\" button to generate a translation in SRT file format.</li> <li>The translated text can also be modified and confirmed with CTRL + ENTER.</li> <li>You can download both the translated version and the original.</li> </ol> <p>\ud83d\udd04 Application Flow</p> <pre><code>A[Upload Video] --&gt; B[Extract Audio (ffmpeg)]\nB --&gt; C[Generate Audio File]\nC --&gt; D[Audio Transcription (OpenAI)]\nD --&gt; E[Editable Transcript]\nE --&gt; F[Translation]\nF --&gt; G[Editable Tranlated Transcript]\nG --&gt; H[SRT File Output]\n</code></pre>"},{"location":"typemachine/#quick-spec","title":"quick-spec:","text":"<p>Python 3.11.11</p>"},{"location":"typemachine/#name-version-build-channel","title":"Name Version Build Channel","text":"<ol> <li>streamlit 1.42.0 py311haa95532_0</li> <li>streamlit-audiorecorder 0.0.6 pypi_0 pypi</li> <li>pydub 0.25.1 pyhd8ed1ab_1 conda-forge</li> <li>python-dotenv 0.21.0 py311haa95532_0</li> <li>openai 2.14.0 pyhd8ed1ab_0 conda-forge</li> <li>ffmpeg 6.1.1 hc79a5da_2</li> <li>ffmpeg-python 0.2.0 pypi_0 pypi</li> </ol>"},{"location":"typemachine/#also-in-use","title":"Also in use:","text":"<p>io, BytesIO, hashlib, md5 os</p>"},{"location":"typemachine/#project-structure","title":"Project Structure","text":"<p>=================</p> <pre><code>app/\n\u2502\n\u251c\u2500\u2500 ai/\n\u2502   \u2514\u2500\u2500 transcrible.py  --&gt; all function needed for recognition the langue of text, transcrible and translate of text\n\u2502\n\u251c\u2500\u2500 audio/\n\u2502   \u2514\u2500\u2500 extract.py  --&gt; function needed for extract mp3 format from Video\n\u2502\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 settings.py  --&gt; hardcode the AI model used in project\n\u2502\n\u2514\u2500\u2500 main.py  --&gt; main code of the app.\n</code></pre> <p>Link to app GitHub</p>"},{"location":"typemachine/#mini-spec","title":"Mini spec:","text":"<ol> <li>Python 3.11.11</li> <li>streamlit 1.42.0 py311haa95532_0</li> <li>streamlit-audiorecorder 0.0.6 pypi_0</li> <li>pypi pydub 0.25.1 pyhd8ed1ab_1 conda-forge</li> <li>python-dotenv 0.21.0 py311haa95532_0 </li> <li>openai 1.47.0 pyhd8ed1ab_0 conda-forge </li> <li>ffmpeg 6.1.1 hc79a5da_2 </li> <li>ffmpeg-python 0.2.0 pypi_0 pypi</li> </ol>"},{"location":"typemachine/#appearance-of-the-application","title":"Appearance of the application","text":""}]}